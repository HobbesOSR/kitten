/*
 * include/asm-x86_64/processor.h
 *
 * Copyright (C) 1994 Linus Torvalds
 */

#ifndef _ARM_64_PROCESSOR_H
#define _ARM_64_PROCESSOR_H

#include <arch/segment.h>
#include <arch/page.h>
#include <arch/types.h>
#include <arch/sigcontext.h>
#include <arch/cpufeature.h>
/* #include <linux/threads.h> */
#include <arch/msr.h>
#include <arch/current.h>
#include <arch/system.h>
/* #include <arch/mmsegment.h> */
#include <arch/percpu.h>
/* #include <lwk/personality.h> */
#include <lwk/cpumask.h>
#include <lwk/cache.h>
#include <arch/fpsimd.h>
#include <arch/msr.h>
#include <arch/types.h>


struct tcr_el1 {
    union {
	u64 val;
	struct {
	    u64 t0sz 	: 6;
	    u64 res0_0	: 1;
	    u64 epd0	: 1;
	    u64 irgn0	: 2;
	    u64 orgn0	: 2;
	    u64 sh0	: 2;
	    u64 tg0	: 2;
	    u64 t1sz	: 6;
	    u64 a1	: 1;
	    u64 epd1	: 1;
	    u64 irgn1	: 2;
	    u64 orgn1	: 2;
	    u64 sh1	: 2;
	    u64 tg1	: 2;
	    u64 ips	: 3;
	    u64 res0_1	: 1;
	    u64 as	: 1;
	    u64 tbi0	: 1;
	    u64 tbi1	: 1;
	};
    };
}__attribute__((packed));

struct midr_el1 {
    union {
	u64 val;
	struct {
	    u64 revision : 4;
	    u64 partnum  : 12;
	    u64 arch     : 4;
	    u64 variant  : 4;
	    u64 vendor   : 8;
	    u64 res0     : 32;
	};
    };
} __attribute__((packed));


struct id_aa64mmfr0_el1 {
    union {
	u64 val;
	struct {
	    u64 pa_range     : 4;  /* Physical Address range */
	    u64 asid_bits    : 4;  /* Number of ASID bits */
	    u64 big_end      : 4;  /* mixed-endian configuration */
	    u64 sns_mem      : 4;  /* distinction between Secure and Non-secure Memory */
	    u64 big_end_el0  : 4;  /* mixed-endian at EL0 only */
	    u64 t_gran_16    : 4;  /* 16KB memory translation granule size */
	    u64 t_gran_64    : 4;  /* 64KB memory translation granule size */
	    u64 t_gran_4     : 4;  /* 4KB  memory translation granule size */
	    u64 t_gran_16_2  : 4;  /* 16KB memory granule size at stage 2  */
	    u64 t_gran_64_2  : 4;  /* 64KB memory granule size at stage 2  */
	    u64 t_gran_4_2   : 4;  /* 4KB  memory granule size at stage 2  */
	    u64 ex_s         : 4;  /* disabling context synchronizing exception entry and exit */
	    u64 res0         : 8; 
	    u64 fgt          : 4;  /* Fine-Grained Trap controls */
	    u64 ecv          : 4;  /* Enhanced Counter Virtualization */
	};
    };

} __attribute__((packed));


struct id_aa64mmfr1_el1 {
    union {
	u64 val;
	struct {
	    u64 hafdbs       : 4;  /* Hardware updates to Access flag and Dirty state */
	    u64 vmid_bits    : 4;  /* Number of VMID bits */
	    u64 vh           : 4;  /* Virtualization Host Extensions */
	    u64 hpds         : 4;  /* Hierarchical Permission Disables */
	    u64 lo           : 4;  /* LORegions */
	    u64 pan          : 4;  /* Privileged Access Never */
	    u64 spec_sei     : 4;  /* PE can generate SError interrupt exceptions from speculative reads */
	    u64 xnx          : 4;  /* execute-never control distinction by Exception level at stage 2 */
	    u64 twed         : 4;  /* configurable delayed trapping of WFE  */
	    u64 ets          : 4;  /* Enhanced Translation Synchronization  */
	    u64 hcx          : 4;  /* support for HCRX_EL2 and its associated EL3 trap  */
	    u64 afp          : 4;  /* support for FPCR.{AH, FIZ, NEP} */
	    u64 n_tlb_pa     : 4;  /* intermediate caching of translation table walks */
	    u64 res0         : 12; 

	};
    };
} __attribute__((packed));


struct id_aa64mmfr2_el1 {
    union {
	u64 val;
	struct {
	    u64 cnp       : 4;  /* Hardware updates to Access flag and Dirty state */
	    u64 uao       : 4;  /* Number of VMID bits */
	    u64 lsm       : 4;  /* Virtualization Host Extensions */
	    u64 iesb      : 4;  /* Hierarchical Permission Disables */
	    u64 va_range  : 4;
	    u64 cc_idx    : 4;  /* LORegions */
	    u64 nv        : 4;  /* Nested Virtualization */
	    u64 st        : 4;  /* small translation tables */
	    u64 at        : 4;  /* unaligned single-copy atomicity and atomic functions */
	    u64 ids       : 4;  /* exception generated by a read access to the feature ID space */
	    u64 fwb       : 4;  /* support for HCR_EL2.FWB  */
	    u64 res0      : 4;  /* support for HCRX_EL2 and its associated EL3 trap  */
	    u64 ttl       : 4;  /* TTL field in address operations */
	    u64 bbm       : 4;  /* break-before-make sequences when changing block size */
	    u64 evt       : 4;  /* Enhanced Virtualization Traps */
	    u64 e0_pd     : 4;  /* E0PD mechanism */

	};
    };
} __attribute__((packed));

struct id_aa64pfr0_el1 {
    union {
	u64 val;
	struct {
	    u64 el0       : 4;  /* EL0 Exception level handling */
	    u64 el1       : 4;  /* EL1 Exception level handling */
	    u64 el2       : 4;  /* EL2 Exception level handling */
	    u64 el3       : 4;  /* EL3 Exception level handling */
	    u64 fp        : 4;  /* Floating-point */
	    u64 adv_simd  : 4;  /* Advanced SIMD */
	    u64 gic       : 4;  /* System register GIC CPU interface */
	    u64 ras       : 4;  /* RAS Extension version */
	    u64 sve       : 4;  /* Scalable Vector Extension */
	    u64 sel2      : 4;  /* Secure EL2 */
	    u64 mpam      : 4;  /* MPAM Extension  */
	    u64 amu       : 4;  /* Activity Monitors Extension  */
	    u64 dit       : 4;  /* Data Independent Timing */
	    u64 res0      : 4;  
	    u64 csv2      : 4;  /* Speculative use of out of context branch targets */
	    u64 csv3      : 4;  /* Speculative use of faulting data */

	};
    };
} __attribute__((packed));

struct id_aa64pfr1_el1 {
    union {
	u64 val;
	struct {
	    u64 bt         : 4;  /* Branch Target Identification mechanism */
	    u64 ssbs       : 4;  /* Speculative Store Bypassing controls */
	    u64 mte        : 4;  /* Memory Tagging Extension */
	    u64 ras_frac   : 4;  /* RAS Extension fractional field */
	    u64 mpam_frac  : 4;  /* MPAM Extension fractional field */
	    u64 res0_0     : 12; 
	    u64 csv2_frac  : 4;  /* CSV2 fractional field */
	    u64 res0_1     : 28; 
	};
    };
} __attribute__((packed));



static inline struct tcr_el1 get_tcr_el1()
{
	struct tcr_el1 tcr;
	__asm__ __volatile__("mrs %0, tcr_el1\n":"=r"(tcr));
	return tcr;
}

static inline unsigned long get_ttbr1_el1()
{
	unsigned long tmp;
	__asm__ __volatile__("mrs %0, ttbr1_el1\n":"=r"(tmp));
	return tmp;
}

static inline unsigned long get_ttbr0_el1()
{
	unsigned long tmp;
	__asm__ __volatile__("mrs %0, ttbr0_el1\n":"=r"(tmp));
	return tmp;
}

struct cpu_context {
    unsigned long x19;
    unsigned long x20;
    unsigned long x21;
    unsigned long x22;
    unsigned long x23;
    unsigned long x24;
    unsigned long x25;
    unsigned long x26;
    unsigned long x27;
    unsigned long x28;
    unsigned long fp;
    unsigned long sp;
    unsigned long pc;
    unsigned long sp0;
    unsigned long usersp;
};

struct thread_struct {
    struct cpu_context  cpu_context;    /* cpu context */
    unsigned long       tp_value;
    struct fpsimd_state fpsimd_state;
    unsigned long       fault_address;  /* fault info */
    //struct debug_info   debug;      /* debugging */
};

#define TF_MASK		0x00000100
#define IF_MASK		0x00000200
#define IOPL_MASK	0x00003000
#define NT_MASK		0x00004000
#define VM_MASK		0x00020000
#define AC_MASK		0x00040000
#define VIF_MASK	0x00080000	/* virtual interrupt flag */
#define VIP_MASK	0x00100000	/* virtual interrupt pending */
#define ID_MASK		0x00200000

#define desc_empty(desc) \
               (!((desc)->a | (desc)->b))

#define desc_equal(desc1, desc2) \
               (((desc1)->a == (desc2)->a) && ((desc1)->b == (desc2)->b))

/*
 * Default implementation of macro that returns current
 * instruction pointer ("program counter").
 */
#define current_text_addr()

//({ void *pc; asm volatile("leaq 1f(%%rip),%0\n1:":"=r"(pc)); pc; })

#define ARM64_VENDOR_RSVD           0x00
#define ARM64_VENDOR_AMPERE         0xc0
#define ARM64_VENDOR_ARM            0x41
#define ARM64_VENDOR_BROADCOM       0x42
#define ARM64_VENDOR_CAVIUM         0x43
#define ARM64_VENDOR_DEC            0x44
#define ARM64_VENDOR_FUJITSU        0x46
#define ARM64_VENDOR_INFINEON       0x49
#define ARM64_VENDOR_FREESCALE      0x4d
#define ARM64_VENDOR_NVIDIA         0x4e
#define ARM64_VENDOR_APPLIED_MICRO  0x50
#define ARM64_VENDOR_QUALCOMM       0x51
#define ARM64_VENDOR_MARVELL        0x56
#define ARM64_VENDOR_INTEL          0x69


#define ARM64_ARCH_ARMV4            0x1
#define ARM64_ARCH_ARMV4T           0x2
#define ARM64_ARCH_ARMV5            0x3
#define ARM64_ARCH_ARMV5T           0x4
#define ARM64_ARCH_ARMV5TE          0x5
#define ARM64_ARCH_ARMV5TEJ         0x6
#define ARM64_ARCH_ARMV6            0x7
#define ARM64_ARCH_EXTENDED         0xf


extern void identify_cpu(void);




#define BOOTSTRAP_THREAD  { \
	.rsp0 = (unsigned long)&bootstrap_stack + sizeof(bootstrap_stack) \
}


#define INIT_MMAP \
{ &init_mm, 0, 0, NULL, PAGE_SHARED, VM_READ | VM_WRITE | VM_EXEC, 1, NULL, NULL }

#define get_debugreg(var, register)

#if 0
\
		__asm__("movq %%db" #register ", %0"		\
			:"=r" (var))
#endif

#define set_debugreg(value, register)

#if 0
			 \
		__asm__("movq %0,%%db" #register		\
			: /* no output */			\
			:"r" (value))
#endif
struct mm_struct;

/* Free all resources held by a thread. */
extern void release_thread(struct task_struct *);

/* Prepare to copy thread state - unlazy all lazy status */
extern void prepare_to_copy(struct task_struct *tsk);

/*
 * create a kernel thread without removing it from tasklists
 */
extern long kernel_thread(int (*fn)(void *), void * arg, unsigned long flags);

/*
 * Return saved PC of a blocked thread.
 * What is this good for? it will be always the scheduler or ret_from_fork.
 */
#define thread_saved_pc(t) (*(unsigned long *)((t)->thread.rsp - 8))

extern unsigned long get_wchan(struct task_struct *p);
#define task_pt_regs(tsk) ((struct pt_regs *)(tsk)->thread.rsp0 - 1)
#define KSTK_EIP(tsk) (task_pt_regs(tsk)->rip)
#define KSTK_ESP(tsk) -1 /* sorry. doesn't work for syscall. */


struct microcode_header {
	unsigned int hdrver;
	unsigned int rev;
	unsigned int date;
	unsigned int sig;
	unsigned int cksum;
	unsigned int ldrver;
	unsigned int pf;
	unsigned int datasize;
	unsigned int totalsize;
	unsigned int reserved[3];
};

struct microcode {
	struct microcode_header hdr;
	unsigned int bits[0];
};

typedef struct microcode microcode_t;
typedef struct microcode_header microcode_header_t;

/* microcode format is extended from prescott processors */
struct extended_signature {
	unsigned int sig;
	unsigned int pf;
	unsigned int cksum;
};

struct extended_sigtable {
	unsigned int count;
	unsigned int cksum;
	unsigned int reserved[3];
	struct extended_signature sigs[0];
};


#define ASM_NOP_MAX 8

/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
static inline void rep_nop(void)
{
	//__asm__ __volatile__("rep;nop": : :"memory");
}

/* Stop speculative execution */
static inline void sync_core(void)
{ 
	int tmp;
	//asm volatile("cpuid" : "=a" (tmp) : "0" (1) : "ebx","ecx","edx","memory");
} 

#define cpu_has_fpu 1

#define ARCH_HAS_PREFETCH
static inline void prefetch(void *x) 
{ 
	//asm volatile("prefetcht0 %0" :: "m" (*(unsigned long *)x));
} 

#define ARCH_HAS_PREFETCHW 1
static inline void prefetchw(void *x) 
{ 
	//asm volatile("prefetchtw %0" :: "m" (*(unsigned long *)x));
} 

#define ARCH_HAS_SPINLOCK_PREFETCH 1

#define spin_lock_prefetch(x)  prefetchw(x)

#define cpu_relax()   rep_nop()

static inline void serialize_cpu(void)
{
	//__asm__ __volatile__ ("cpuid" : : : "ax", "bx", "cx", "dx");
}


#define stack_current() \
({								\
	struct thread_info *ti;					\
	/*asm("andq %%rsp,%0; ":"=r" (ti) : "0" (CURRENT_MASK));*/	\
	ti->task;					\
})

#define cache_line_size() (boot_cpu_data.arch.x86_cache_alignment)

extern unsigned long boot_option_idle_override;
/* Boot loader type from the setup header */
extern int bootloader_type;

#define HAVE_ARCH_PICK_MMAP_LAYOUT 1

#endif /* _ARM_64_PROCESSOR_H */
